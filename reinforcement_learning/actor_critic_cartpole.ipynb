{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaca1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\core\\framework\\tensor_shape_pb2.py:23: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"z\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32 .tensorflow.TensorShapeProto.Dim\\x12\\x14\\n\\x0cunknown_rank\\x18\\x03 \\x01(\\x08\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tBq\\n\\x18org.tensorflow.frameworkB\\x11TensorShapeProtosP\\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\\xf8\\x01\\x01\\x62\\x06proto3')\n",
      "C:\\Users\\vaca1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\core\\framework\\tensor_shape_pb2.py:42: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  serialized_options=None, file=DESCRIPTOR),\n",
      "C:\\Users\\vaca1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\core\\framework\\tensor_shape_pb2.py:63: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  serialized_end=182,\n",
      "C:\\Users\\vaca1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\core\\framework\\types_pb2.py:24: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  serialized_pb=_b('\\n%tensorflow/core/framework/types.proto\\x12\\ntensorflow*\\xaa\\x06\\n\\x08\\x44\\x61taType\\x12\\x0e\\n\\nDT_INVALID\\x10\\x00\\x12\\x0c\\n\\x08\\x44T_FLOAT\\x10\\x01\\x12\\r\\n\\tDT_DOUBLE\\x10\\x02\\x12\\x0c\\n\\x08\\x44T_INT32\\x10\\x03\\x12\\x0c\\n\\x08\\x44T_UINT8\\x10\\x04\\x12\\x0c\\n\\x08\\x44T_INT16\\x10\\x05\\x12\\x0b\\n\\x07\\x44T_INT8\\x10\\x06\\x12\\r\\n\\tDT_STRING\\x10\\x07\\x12\\x10\\n\\x0c\\x44T_COMPLEX64\\x10\\x08\\x12\\x0c\\n\\x08\\x44T_INT64\\x10\\t\\x12\\x0b\\n\\x07\\x44T_BOOL\\x10\\n\\x12\\x0c\\n\\x08\\x44T_QINT8\\x10\\x0b\\x12\\r\\n\\tDT_QUINT8\\x10\\x0c\\x12\\r\\n\\tDT_QINT32\\x10\\r\\x12\\x0f\\n\\x0b\\x44T_BFLOAT16\\x10\\x0e\\x12\\r\\n\\tDT_QINT16\\x10\\x0f\\x12\\x0e\\n\\nDT_QUINT16\\x10\\x10\\x12\\r\\n\\tDT_UINT16\\x10\\x11\\x12\\x11\\n\\rDT_COMPLEX128\\x10\\x12\\x12\\x0b\\n\\x07\\x44T_HALF\\x10\\x13\\x12\\x0f\\n\\x0b\\x44T_RESOURCE\\x10\\x14\\x12\\x0e\\n\\nDT_VARIANT\\x10\\x15\\x12\\r\\n\\tDT_UINT32\\x10\\x16\\x12\\r\\n\\tDT_UINT64\\x10\\x17\\x12\\x10\\n\\x0c\\x44T_FLOAT_REF\\x10\\x65\\x12\\x11\\n\\rDT_DOUBLE_REF\\x10\\x66\\x12\\x10\\n\\x0c\\x44T_INT32_REF\\x10g\\x12\\x10\\n\\x0c\\x44T_UINT8_REF\\x10h\\x12\\x10\\n\\x0c\\x44T_INT16_REF\\x10i\\x12\\x0f\\n\\x0b\\x44T_INT8_REF\\x10j\\x12\\x11\\n\\rDT_STRING_REF\\x10k\\x12\\x14\\n\\x10\\x44T_COMPLEX64_REF\\x10l\\x12\\x10\\n\\x0c\\x44T_INT64_REF\\x10m\\x12\\x0f\\n\\x0b\\x44T_BOOL_REF\\x10n\\x12\\x10\\n\\x0c\\x44T_QINT8_REF\\x10o\\x12\\x11\\n\\rDT_QUINT8_REF\\x10p\\x12\\x11\\n\\rDT_QINT32_REF\\x10q\\x12\\x13\\n\\x0f\\x44T_BFLOAT16_REF\\x10r\\x12\\x11\\n\\rDT_QINT16_REF\\x10s\\x12\\x12\\n\\x0e\\x44T_QUINT16_REF\\x10t\\x12\\x11\\n\\rDT_UINT16_REF\\x10u\\x12\\x15\\n\\x11\\x44T_COMPLEX128_REF\\x10v\\x12\\x0f\\n\\x0b\\x44T_HALF_REF\\x10w\\x12\\x13\\n\\x0f\\x44T_RESOURCE_REF\\x10x\\x12\\x12\\n\\x0e\\x44T_VARIANT_REF\\x10y\\x12\\x11\\n\\rDT_UINT32_REF\\x10z\\x12\\x11\\n\\rDT_UINT64_REF\\x10{Bk\\n\\x18org.tensorflow.frameworkB\\x0bTypesProtosP\\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\\xf8\\x01\\x01\\x62\\x06proto3')\n",
      "C:\\Users\\vaca1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\core\\framework\\types_pb2.py:36: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  type=None),\n",
      "C:\\Users\\vaca1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\core\\framework\\types_pb2.py:225: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  serialized_end=864,\n",
      "C:\\Users\\vaca1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\core\\framework\\resource_handle_pb2.py:27: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  dependencies=[tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2.DESCRIPTOR,])\n",
      "C:\\Users\\vaca1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\core\\framework\\resource_handle_pb2.py:45: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  serialized_options=None, file=DESCRIPTOR),\n",
      "C:\\Users\\vaca1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\core\\framework\\resource_handle_pb2.py:66: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  serialized_end=436,\n",
      "C:\\Users\\vaca1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\core\\framework\\tensor_pb2.py:28: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  dependencies=[tensorflow_dot_core_dot_framework_dot_resource__handle__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2.DESCRIPTOR,])\n",
      "C:\\Users\\vaca1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\core\\framework\\tensor_pb2.py:46: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  serialized_options=None, file=DESCRIPTOR),\n",
      "C:\\Users\\vaca1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\core\\framework\\tensor_pb2.py:172: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  serialized_end=713,\n",
      "C:\\Users\\vaca1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\core\\framework\\attr_value_pb2.py:28: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  dependencies=[tensorflow_dot_core_dot_framework_dot_tensor__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2.DESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2.DESCRIPTOR,])\n",
      "C:\\Users\\vaca1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\core\\framework\\attr_value_pb2.py:46: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  serialized_options=None, file=DESCRIPTOR),\n",
      "C:\\Users\\vaca1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\core\\framework\\attr_value_pb2.py:109: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  serialized_end=725,\n",
      "C:\\Users\\vaca1\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:506: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "  f\"The environment {path} is out of date. You should consider \"\n",
      "C:\\Users\\vaca1\\Anaconda3\\lib\\site-packages\\gym\\core.py:173: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
      "  \"Function `env.seed(seed)` is marked as deprecated and will be removed in the future. \"\n"
     ]
    }
   ],
   "source": [
    "import gym # Used the \"pip install pygame\" to get it working\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Configuration parameters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "max_steps_per_episode = 10000\n",
    "env = gym.make(\"CartPole-v0\")  # Create the environment\n",
    "env.seed(seed)\n",
    "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaca1\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "num_hidden = 128\n",
    "\n",
    "inputs = layers.Input(shape=(num_inputs,))\n",
    "common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "critic = layers.Dense(1)(common)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 11.76 at episode 10\n",
      "running reward: 18.71 at episode 20\n",
      "running reward: 16.17 at episode 30\n",
      "running reward: 17.66 at episode 40\n",
      "running reward: 15.92 at episode 50\n",
      "running reward: 16.49 at episode 60\n",
      "running reward: 16.45 at episode 70\n",
      "running reward: 15.79 at episode 80\n",
      "running reward: 16.72 at episode 90\n",
      "running reward: 15.95 at episode 100\n",
      "running reward: 16.11 at episode 110\n",
      "running reward: 17.90 at episode 120\n",
      "running reward: 17.87 at episode 130\n",
      "running reward: 22.21 at episode 140\n",
      "running reward: 27.88 at episode 150\n",
      "running reward: 28.62 at episode 160\n",
      "running reward: 32.76 at episode 170\n",
      "running reward: 60.40 at episode 180\n",
      "running reward: 111.93 at episode 190\n",
      "running reward: 144.71 at episode 200\n",
      "running reward: 105.44 at episode 210\n",
      "running reward: 87.10 at episode 220\n",
      "running reward: 89.37 at episode 230\n",
      "running reward: 120.91 at episode 240\n",
      "running reward: 113.15 at episode 250\n",
      "running reward: 111.17 at episode 260\n",
      "running reward: 119.73 at episode 270\n",
      "running reward: 113.89 at episode 280\n",
      "running reward: 111.01 at episode 290\n",
      "running reward: 126.69 at episode 300\n",
      "running reward: 145.94 at episode 310\n",
      "running reward: 164.31 at episode 320\n",
      "running reward: 167.10 at episode 330\n",
      "running reward: 167.02 at episode 340\n",
      "running reward: 168.50 at episode 350\n",
      "running reward: 168.54 at episode 360\n",
      "running reward: 168.45 at episode 370\n",
      "running reward: 171.16 at episode 380\n",
      "running reward: 147.10 at episode 390\n",
      "running reward: 168.08 at episode 400\n",
      "running reward: 180.89 at episode 410\n",
      "running reward: 188.56 at episode 420\n",
      "running reward: 193.15 at episode 430\n",
      "running reward: 194.86 at episode 440\n",
      "Solved at episode 441!\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "huber_loss = keras.losses.Huber()\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "rewards_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "while True:  # Run until solved\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            #env.render(); #Adding this line would show the attempts\n",
    "            # of the agent in a pop up window.\n",
    "\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "\n",
    "            # Predict action probabilities and estimated future rewards\n",
    "            # from environment state\n",
    "            action_probs, critic_value = model(state)\n",
    "            critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "            # Sample action from action probability distribution\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "\n",
    "            # Apply the sampled action in our environment\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Update running reward to check condition for solving\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # Calculate expected value from rewards\n",
    "        # - At each timestep what was the total reward received after that timestep\n",
    "        # - Rewards in the past are discounted by multiplying them with gamma\n",
    "        # - These are the labels for our critic\n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[::-1]:\n",
    "            discounted_sum = r + gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "\n",
    "        # Normalize\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "        returns = returns.tolist()\n",
    "\n",
    "        # Calculating loss values to update our network\n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, ret in history:\n",
    "            # At this point in history, the critic estimated that we would get a\n",
    "            # total reward = `value` in the future. We took an action with log probability\n",
    "            # of `log_prob` and ended up recieving a total reward = `ret`.\n",
    "            # The actor must be updated so that it predicts an action that leads to\n",
    "            # high rewards (compared to critic's estimate) with high probability.\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "            # The critic must be updated so that it predicts a better estimate of\n",
    "            # the future rewards.\n",
    "            critic_losses.append(\n",
    "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "            )\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Clear the loss and reward history\n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "\n",
    "    # Log details\n",
    "    episode_count += 1\n",
    "    if episode_count % 10 == 0:\n",
    "        template = \"running reward: {:.2f} at episode {}\"\n",
    "        print(template.format(running_reward, episode_count))\n",
    "\n",
    "    if running_reward > 195:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful resources:\n",
    "- https://keras.io/examples/rl/actor_critic_cartpole/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
